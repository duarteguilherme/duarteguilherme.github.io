<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Guilherme Duarte</title>
    <link>/tags/r/index.xml</link>
    <description>Recent content in R on Guilherme Duarte</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="/tags/r/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Should we call Cpp from R? Implementing and benchmarking a simple gradient descent procedure with Rcpp</title>
      <link>/2017/03/15/should-we-call-cpp-from-r-implementing-and-benchmarking-a-simple-gradient-descent-procedure-with-rcpp/</link>
      <pubDate>Wed, 15 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/03/15/should-we-call-cpp-from-r-implementing-and-benchmarking-a-simple-gradient-descent-procedure-with-rcpp/</guid>
      <description>&lt;!-- BLOGDOWN-HEAD --&gt;
&lt;!-- /BLOGDOWN-HEAD --&gt;

&lt;!-- BLOGDOWN-BODY-BEFORE --&gt;
&lt;!-- /BLOGDOWN-BODY-BEFORE --&gt;
&lt;p&gt;In this post, we’ll implement a simple gradient descent function in c++ directly inside R, employing the package Rcpp. This post intends to show how sometimes c++ can be used to make code faster in some circumstances, so we encourage people to code more in this language.&lt;/p&gt;
&lt;p&gt;So let’s get down to brass tacks. Gradient descent is an algorithm for optimization of functions with low peaks. This procedures tries to get to the local minimum of a function by iterating through the the proportional gradient of this function (if you don’t get it, try Calculus 101, or read it on &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_descent&#34;&gt;Wikipedia&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Suppose a common logistic regression application. Let’s simulate some binary data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 1250000
x &amp;lt;- rnorm(n)
y &amp;lt;- 1/(1 + exp(-1.3 - 1.6 * x - rnorm(n)))
y &amp;lt;- rbinom(n,1,y)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the purpose of generating estimates for this logit regression, we need a cross-entropy function: &lt;span class=&#34;math display&#34;&gt;\[D(S, L) = - \sum_{i=1}^n L_i * log(S_i)\]&lt;/span&gt;, such that &lt;span class=&#34;math display&#34;&gt;\[S_i = logit(ax_i +b)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the context of &lt;a href=&#34;https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression&#34;&gt;logistic regression&lt;/a&gt;: &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
D(S,L) =   \\ -y log(S_i) - (1 - y)  log(1 - S_i)
\end{equation}\]&lt;/span&gt;&lt;br /&gt;
Now we need to minimize the average cross-entropy function: &lt;span class=&#34;math display&#34;&gt;\[L =   \frac{1}{N} D(S,L)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We need the gradient for this function. We take for granted that &lt;span class=&#34;math display&#34;&gt;\[\frac{\partial L}{\partial b} = \frac{1}{2N}  \sum_{i=1}^{n} (S_i - y_i)\]&lt;/span&gt; and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial L}{\partial a} = \frac{1}{2N} \sum_{i=1}^{n} (S_i - y_i)x_i\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;implementing-a-gradient-descent-function-in-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Implementing a Gradient Descent Function in R&lt;/h3&gt;
&lt;p&gt;In order to compare, we’re implementing a Gradient Descent Function in R firstly. This function will receive data, a threshold, and values for a and b. Next, those functions will be updated until their difference were lesser than the threshold. Another important information: no for loops here, we’re using recursion.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gd_r &amp;lt;- function(y,x, threshold, a, b, lr) {
  n &amp;lt;- length(y)
  
  # Updating values for a and b
  a_n &amp;lt;- a - lr*(1/(2*n))*sum( (1/(1+exp( -a*x - b)) - y)*x)
  b_n &amp;lt;- b - lr*(1/(2*n))*sum( 1/(1+exp( -a*x - b)) - y)

  # Test if the difference between old and new values 
  #   for a and b reached the threshold
  if( abs(a_n - a) &amp;gt; threshold || abs(b_n - b) &amp;gt; threshold ) {
    # Recursion
    return(gd_r(y,x,threshold,a_n,b_n, lr))
  } 
  return(c(a_n, b_n))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check if it’s working for a threshold of .0001, a learning rate of .9, and initial values for a and b, .5.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  gd_r(y,x,.0001, .5,.5,.9) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.365905 1.107018&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;implementing-gradient-descent-in-c&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Implementing Gradient Descent in C++&lt;/h3&gt;
&lt;p&gt;Now I’ll implement the same function, but in c++ directly in R. This can be done using the package Rcpp. For more references, check out &lt;a href=&#34;http://adv-r.had.co.nz/Rcpp.html&#34;&gt;Hadley Wickham book&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We just need to define our c++ function inside the Rcpp function cppFunction. (If you don’t know c++, don’t bother, Hadley book on R is a good introduction.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Rcpp)
cppFunction(&amp;#39;
NumericVector gd_cpp(IntegerVector y, NumericVector x, double threshold, double a, double b, double lr) {
  NumericVector ab(2);

  // n will represent the length of y and x
  int n = y.size();

  // In this case, we re using for loops to calculate sums. That s right, no Rcpp:sum here.
  double sum_a = 0;
  double sum_b = 0;
  for(int i = 0; i &amp;lt; n; i++) {
    sum_a += (1/(1+std::exp( -a*x[i] - b)) - y[i])*x[i];
    sum_b += (1/(1+std::exp( -a*x[i] - b)) - y[i]);
  }

  // Updating values for a and b
  ab[0] = a - lr * sum_a / (2*n);
  ab[1] = b - lr * sum_b / (2*n);

  // Recursion
  if ( std::abs(ab[0] - a) &amp;gt; threshold || std::abs(ab[1] - b) &amp;gt; threshold ) {
    return gd_cpp(y,x,threshold, ab[0],ab[1],lr);
  }
  return ab;
  
}
&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check if it’s working.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gd_cpp(y,x,.0001, .5,.5,.9) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.365905 1.107018&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks great!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-perfomance-of-both-functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comparing perfomance of both functions&lt;/h3&gt;
&lt;p&gt;Now we have to check if the function we create in c++ outperform R function. For this operation, we just have to use the function microbenchmark from “microbenchmark” package. We will run the same operations 50 times. So, we’ll change the learning rate to 2 and the threshold to .001.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(microbenchmark)
res &amp;lt;- microbenchmark(gd_cpp(y,x,.001, .5,.5,2) , gd_r(y,x,.001, .5,.5,2), times=50 )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the winner is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(res)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: seconds
##                              expr      min       lq     mean   median
##  gd_cpp(y, x, 0.001, 0.5, 0.5, 2) 2.886451 2.904519 3.056539 2.994356
##    gd_r(y, x, 0.001, 0.5, 0.5, 2) 4.083224 4.149808 4.657743 4.444425
##        uq     max neval
##  3.105450 3.93215    50
##  5.083942 6.01895    50&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, the function written in cpp was around 1.58 times faster than the same function wwritten in R (3.05 vs 4.83). In other words, if you have an analysis that requires velocity and you don’t bother of writing in a more complex language, you should implement in with Rcpp.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>